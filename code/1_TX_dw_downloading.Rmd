---
title: "Texas Drinking Water Data Downloading"
author: "EmmaLi Tsai"
date: "2024-01-29"
last_updated: "2024-04-10"
output: html_document
---

Packages: 
```{r packages}
library(tidyverse)
library(leaflet)
library(sf)
library(aws.s3) 
library(janitor)
library(tidycensus)
library(areal)
library(mapview)
library(httr)
library(readxl)
library(jsonlite)
library(googlesheets4)
library(tigris)
options(tigris_use_cache = TRUE)
# remotes::install_github("environmental-policy-innovation-center/stateDW", 
#                         auth_token = "insert-github-pat-here")
library(stateDW)

# Store keys on local machine in .Renviron file! 
```

Functions: 
```{r functions}
# Migrated to R package workflow but keeping these here as a backup 
# source("./code/functions/get_sab.R")
# source("./code/functions/get_SDWIS.R")
# source("./code/functions/interp_sab_census.R")
```

_______________________________________________________________________________
### Data Download - code for web scraping & saving raw files. Data here either take a long time to download, require extra tidying, are updated frequently (i.e., reservoir or aquifer levels) or need to be manually downloaded from a website. Everything in this section is added to S3.
_______________________________________________________________________________
## Demographic
```{r data collection}
# SOP for TX-specific data: ####################################################
# PWSIDs should have "TX" at the beginning of them 
# All data must have either pwsid, aquifer, census id (block group/tract), and 
# well number (or any match-able columns)
# Group data collections by category (demographic, environmental, financial, etc. )
# All data should be in wide format 
# All data should have a pulled link to note where it came from
# Add data to the TX_data list 
# Variable naming convention - janitor::clean_names()

## Demographic #################################################################
# grabbing sabs: 
sab <- get_sab(states = c("TX"), crs = ("EPSG:32138"))

# census data - grabbing at the tract level
census_vars <- c(total_pop = "B01003_001",
                 # race and ethnicity stats: 
                 black_alone = "B02001_003", 
                 asian_alone = "B02001_005", 
                 white_alone = "B02001_002", 
                 AIAN_alone = "B02001_004", 
                 NAPI_alone = "B02001_006", 
                 other_alone = "B02001_007", 
                 mixed_alone = "B02001_008",
                 hisp_alone = "B03003_003", 
                 # MHI
                 mhi = "B19013_001", 
                 # labor force and unemployment: 
                 laborforce = "B23025_003",  # universe: pop > 16
                 laborforce_unemployed = "B23025_005", # universe: pop > 16
                 # households below poverty level: 
                 hh_total = "B17017_001",
                 hh_below_pov = "B17017_002", 
                 # age cats: 
                 ageunder_5  = "B06001_002",
                 age5_17 = "B06001_003", 
                 age18_24 = "B06001_004",
                 age25_34 = "B06001_005", 
                 age35_44 = "B06001_006", 
                 age45_54 = "B06001_007", 
                 age55_59 = "B06001_008", 
                 age60_61 = "B06001_009",
                 # language spoken at home: 
                 only_english = "B99162_002",  # universe: pop >5
                 other_lang = "B99162_003", # universe: pop >5
                 # education categories: 
                 no_school = "B15003_002", 
                 high_school = "B15003_017", 
                 bachelors = "B15003_022",
                 prof_degree = "B15003_024", 
                 # nationality: 
                 foreign = "B99051_005")
census <- tidycensus::get_acs(
  geography = "tract", 
  variables = census_vars, 
  # grab stats for neighboring states since the crosswalk
  # uses them for SABs close to the border: 
  state = c("TX", "AR", "OK", "NM", "LA"), 
  year = 2020,
  geometry = TRUE
)

# adding counties: 
census_tidy <- census
counties <- unlist(strsplit(census_tidy$NAME, split = ","))
census_tidy$counties <- trimws(counties[grepl("County|Parish", counties)])

# adding state: 
census_tidy <- census_tidy %>%
  mutate(state = case_when(
    str_detect(NAME, "Arkansas") ~ "Arkansas", 
    str_detect(NAME, "Louisiana") ~ "Louisiana", 
    str_detect(NAME, "Oklahoma") ~ "Oklahoma", 
    str_detect(NAME, "New Mexico") ~ "New_Mexico", 
    str_detect(NAME, "Texas") ~ "Texas"
  ))


# grabbing pwsids in or close to the border of TX, since some
# state_code == "TX" don't exist in TX: 
tx_only <- census_tidy %>% 
  filter(state == "Texas") 
tx <- st_union(tx_only) %>%
  st_buffer(., dist = 0.01) 
sab_sf <- st_transform(sab, crs = st_crs(tx))
tx_sabs <- st_intersection(sab_sf, tx)
# NOTE: there are ~ four pwsids (one of which starts with "NY") that have 
# actual boundaries in TX. 

# filtering for intersected SABs to retain original geographies: 
all_tx_sabs <- sab_sf %>% 
  filter(pwsid %in% tx_sabs$pwsid)
  # st_transform(., crs = st_crs(sab))
pwsids <- unique(all_tx_sabs$pwsid)


# creating a flag for when the county is within East TX: 
east_tx <- read.csv("./data/raw/east_TX_counties.csv") %>%
  select(-X) %>%
  mutate(county_tidy = paste0(county_name, " County"))
# running an intersection with east TX geography
east_tx_geo <- census_tidy %>%
  filter(counties %in% east_tx$county_tidy) %>%
  filter(state == "Texas") %>%
  st_union()
east_tx_sabs <- st_intersection(all_tx_sabs, east_tx_geo)
east_tx_pwsids <- unique(east_tx_sabs$pwsid)
# creating flag: 
sab_flag <- all_tx_sabs %>%
  mutate(east_tx_flag = case_when(
    pwsid %in% east_tx_pwsids ~ "yes",
    TRUE ~ "no")
  ) %>%
  relocate(east_tx_flag, .after = "county_served") %>%
  st_transform(., crs = ("EPSG:32138"))


###############################################################################
# Start of tiered crosswalk approach: 
###############################################################################
# pivoting census to wide format: 
tx_wide <- pivot_wider(census_tidy, 
                       names_from = c("variable"), 
                       values_from = c("estimate", "moe")) %>%
  select(GEOID, estimate_total_pop:estimate_other_lang) %>%
  st_transform(., crs = "EPSG:32138") %>%
  mutate(mhi_percent_uni = estimate_mhi / sum(tx_wide$estimate_mhi, na.rm = T))

# tract crosswalk for tier 2: 
tract_crosswalk <- aws.s3::s3read_using(read.csv, 
                                         object = "s3://tech-team-data/pws_crosswalk/pws_census_tract_weighted_crosswalk.csv") %>%
  select(-X) %>%
  filter(pwsid %in% pwsids) %>%
  # if the geoid is missing prefix 0, add it back in
  mutate(tract_geoid = as.character(tract_geoid),
         tract_geoid = case_when(
           str_length(tract_geoid) == 10 ~ paste0(0, tract_geoid),
           TRUE ~ tract_geoid
         )) %>%
  # there are 7 tract geoids in the crosswalk are NAs - assuming these are errors?
  filter(!(is.na(tract_geoid)))

# loading block weights for pw_interpolation: 
tx_blocks <- tigris::blocks(
  state = "TX", 
  year = 2020) %>%
  st_transform(., crs = "EPSG:32138")

################################################################################
## Tier 1: Population-weighted interpolation: 
################################################################################
# pw interpolation on count vars where the universe is total population: 
tx_wide_pop <- tx_wide %>% 
  select(estimate_total_pop:estimate_other_lang) %>%
  # leaving these columns out because their universe is different
  select(-c(estimate_hh_total, estimate_hh_below_pov, 
            estimate_mhi))

pw_inter_pop <- interpolate_pw(
  from = tx_wide_pop,
  to = sab,
  to_id = "pwsid",
  extensive = TRUE, 
  weights = tx_blocks,
  weight_column = "POP20",
  crs = 32138)

## Households below poverty: 
tx_wide_hh <- tx_wide %>% 
  select(estimate_hh_total, estimate_hh_below_pov)

# since we're using household data, should we use housing20 as weights
pw_inter_hh <- interpolate_pw(
  from = tx_wide_hh,
  to = sab,
  to_id = "pwsid",
  extensive = TRUE, 
  weights = tx_blocks,
  weight_column = "HOUSING20",
  crs = 32138)

pwsid_hh <- pw_inter_hh %>%
  mutate(estimate_hh_below_pov_per = 100*(estimate_hh_below_pov/estimate_hh_total)) 

## Median household income: 
tx_wide_mhi <- tx_wide %>% 
  select(mhi_percent_uni)

# pop weighted interpolation on median household income: 
pw_interp_mhi <- interpolate_pw(
  from = tx_wide_mhi,
  to = sab,
  to_id = "pwsid",
  extensive = FALSE, 
  weights = tx_blocks,
  weight_column = "POP20",
  crs = 32138
)

# transforming percentage of universe to actual mhi: 
pwsid_mhi <- pw_interp_mhi %>%
  mutate(estimate_mhi = mhi_percent_uni*sum(tx_wide$estimate_mhi,
                                            na.rm = T)) 

## Bringing everything back together: 
pwsid_mhi_df <- pwsid_mhi %>%
  as.data.frame() %>%
  select(-geom)

pwsid_hh_df <- pwsid_hh %>%
  as.data.frame() %>%
  select(-geom)

# all census data: 
all_census <- pw_inter_pop %>%
  left_join(pwsid_mhi_df) %>%
  left_join(pwsid_hh_df)

# filtering NAs for tier 1: 
pw <- all_census %>%
  filter(!(is.na(estimate_total_pop) | estimate_total_pop == 0)) %>%
  mutate(tier_crosswalk = "tier_1")

# calculating percentages: 
pw_per <- pw %>%
  as.data.frame() %>%
  select(-geom) %>%
  select(starts_with(c("estimate", "pwsid"))) %>%
  mutate_at(vars(!c("estimate_total_pop", "estimate_mhi", "pwsid", 
                    "estimate_hh_total", "estimate_hh_below_pov", 
                    "estimate_laborforce", "estimate_laborforce_unemployed", 
                    "estimate_hh_below_pov_per")), 
            ~((./estimate_total_pop)*100)) %>%
  relocate("pwsid") %>%
  select(-c(estimate_hh_below_pov_per, estimate_hh_below_pov, 
            estimate_mhi, estimate_total_pop, 
            estimate_laborforce_unemployed, estimate_hh_total))
  # mutate(estimate_poc_alone = (100 - estimate_white_alone)) 
colnames(pw_per) <- paste0(colnames(pw_per), "_per")

# merging percentages back with original counts and organizing columns: 
pw_final <- merge(pw, pw_per,
                  by.x = "pwsid", by.y = "pwsid_per") %>%
  # recalculating % households in poverty and % labor force unemployed
  mutate(estimate_hh_below_pov_per = (estimate_hh_below_pov/estimate_hh_total*100), 
         estimate_laborforce_unemployed_per = (estimate_laborforce_unemployed/estimate_laborforce)*100) %>%
  relocate(tier_crosswalk, .after = pwsid) %>%
  relocate(estimate_laborforce_unemployed_per, .after = estimate_other_lang_per) %>%
  relocate(estimate_hh_below_pov_per, .after = estimate_laborforce_unemployed_per) %>%
  relocate(estimate_laborforce:estimate_laborforce_unemployed, .after = estimate_other_lang)

################################################################################
## Tier 2: crosswalking 
################################################################################
# locating the pwsids that fell out of pw interpolation: 
xwalk <- all_census %>%
  filter(is.na(estimate_total_pop) | estimate_total_pop == 0) 

# data prep: 
pwsid_cross <- tract_crosswalk %>%
  filter(pwsid %in% xwalk$pwsid) 
 
tx_wide_cross <- tx_wide %>%
  select(GEOID, estimate_total_pop:estimate_other_lang)


# merging tx census stats with crosswalk: 
pwsid_xwalk <- merge(tx_wide_cross, pwsid_cross, 
                      by.x = "GEOID", by.y = "tract_geoid", all.y = T)

# multiplying census stats by tract weights:
pwsid_weight_xwalk <- pwsid_xwalk %>% 
  mutate(across(estimate_total_pop:estimate_hh_below_pov, ~.*tract_parcel_weight)) %>%
  mutate(across(estimate_laborforce:estimate_other_lang, ~.*tract_parcel_weight))

# sum across pwsid and estimate mhi using a weighted mean: 
pwsid_weighted_xwalk <- pwsid_weight_xwalk %>%
  group_by(pwsid) %>% 
  summarize(across(estimate_total_pop:estimate_hh_below_pov, ~round(sum(.), 2)), 
            estimate_mhi = weighted.mean(estimate_mhi, tract_parcel_weight, na.rm = TRUE), 
            across(estimate_laborforce:estimate_other_lang, ~round(sum(.), 2))) 


# calculating percentage: 
pwsid_weighted_per <- pwsid_weighted_xwalk %>%
  as.data.frame() %>%
  select(-geometry) %>%
  select(starts_with(c("estimate", "pwsid"))) %>%
  mutate_at(vars(!c("estimate_total_pop", "estimate_mhi", "pwsid")), 
            ~((./estimate_total_pop)*100)) %>%
  relocate("pwsid") 
  # mutate(estimate_poc_alone = (100 - estimate_white_alone)) 
colnames(pwsid_weighted_per) <- paste0(colnames(pwsid_weighted_per), "_per")

# merging percentages back with original counts: 
pwsid_xwalked <- merge(pwsid_weighted_xwalk, pwsid_weighted_per,
                       by.x = "pwsid", by.y = "pwsid_per") %>%
  select(-c("estimate_total_pop_per", "estimate_mhi_per")) %>%
  select(-c("estimate_hh_total_per")) %>%
  # recalculating % households in poverty and % labor force unemployed
  mutate(estimate_hh_below_pov_per = (estimate_hh_below_pov/estimate_hh_total*100), 
         estimate_laborforce_unemployed_per = (estimate_laborforce_unemployed/estimate_laborforce)*100)


# want to cap max total pop to those reported by SDWIS: 
sdwis_pop <- tx_sabs %>%
  as.data.frame() %>%
  select(-geom) %>%
  select(pwsid, population_served_count)

sdwis_xwalk <- pwsid_xwalked %>%
  left_join(sdwis_pop) %>%
  relocate(population_served_count, .before = estimate_total_pop) 
  
# capping max pop at SDWIS values: 
sdwis_pop <- sdwis_xwalk %>%
  filter(estimate_total_pop > population_served_count) 

sdwis_counts <- sdwis_pop %>%
  as.data.frame() %>%
  select(-geometry) %>%
  pivot_longer(., estimate_white_alone_per:estimate_other_lang_per) %>%
  select(pwsid, population_served_count, estimate_total_pop, name, value) %>%
  mutate(raw_count = (value/100) * population_served_count, 
         col_name = gsub("_per", "", name)) %>%
  select(pwsid, population_served_count, estimate_total_pop, raw_count, col_name) %>%
  pivot_wider(., names_from = col_name, values_from = raw_count) %>%
  mutate(estimate_total_pop = population_served_count,
         # we can no longer estimate number of households using this method
         estimate_hh_total = NA,
         tier_crosswalk = "tier_2_sdiwspop")

# combining this back with percentages: 
sdwis_per <- sdwis_pop %>% 
  select(pwsid, estimate_mhi, 
         estimate_white_alone_per:estimate_other_lang_per) 

sdwis_pop_final <- merge(sdwis_counts, sdwis_per, by = "pwsid")


# recombining with other pwsids where xwalk < SDWIS: 
xwalk_pop <-  sdwis_xwalk %>%
  filter(estimate_total_pop < population_served_count) %>%
  mutate(tier_crosswalk = "tier_2_xwalkpop")

sdwis_pop_final <- sdwis_pop_final[, names(xwalk_pop)]

final_xwalk <- rbind(sdwis_pop_final, xwalk_pop) %>%
  select(-population_served_count) %>%
  mutate(mhi_percent_uni = NA) %>%
  st_as_sf() %>%
  st_transform(., st_crs(pw))


################################################################################
## Combining tier 1 and 2 methods & tier 3 designation: 
################################################################################
final_xwalk <- final_xwalk[, names(pw_final)]
final_pw_xwalk <- rbind(pw_final, final_xwalk)

# just need to combine with missing pwsids, with NAs for all columns 
missing <- sab_flag %>%
  filter(!(pwsid %in% final_pw_xwalk$pwsid))

# adding these back in, but as tier 3: 
missing_df <- final_pw_xwalk[nrow(final_pw_xwalk)+1:nrow(missing),] 
missing_df$pwsid <- missing$pwsid 
missing_df <- missing_df %>%
  as.data.frame() %>%
  select(-geometry) %>%
  mutate(tier_crosswalk = "tier_3")

# adding back geometries 
missing_sab_geoms <- missing %>%
  select(pwsid)

missing_sf <- missing_df %>%
  left_join(missing_sab_geoms) %>%
  st_as_sf() %>%
  st_transform(., crs = st_crs(final_pw_xwalk))

# completing crosswalk: 
complete_crosswalk <- rbind(final_pw_xwalk, missing_sf)
complete_crosswalk_df <- complete_crosswalk %>% 
  mutate(estimate_poc_alone_per = 100 - estimate_white_alone_per) %>%
  relocate(estimate_poc_alone_per, .after = estimate_mixed_alone_per) %>%
  as.data.frame() %>%
  select(-c(geometry, mhi_percent_uni))

# merge back with sab information: 
final_sab <- merge(sab_flag, complete_crosswalk_df, 
                   by = "pwsid")

# SAB & Census - writing this to s3: 
tmp <- tempfile()
st_write(final_sab, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/raw/TX_sab_census.geojson",
  bucket = "tech-team-data",
)
```

## Environmental
```{r}
## Environmental ###############################################################
## Groundwater database: https://www.twdb.texas.gov/groundwater/data/GWDBDownload.zip
# Pulled Jan 22 2024 
file_loc <- "./data/raw/TX_GWDB"
download.file("https://www.twdb.texas.gov/groundwater/data/GWDBDownload.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
## TODO: add code that downloads & uploads entire folder to aws (HOLD 
# on this since we don't actually use this dataset in the analysis)

## Reservoir levels: https://www.waterdatafortexas.org/reservoirs/statewide
# downloaded Jan 24 2024 
# NOTE: only recent conditions - update more frequently?
res_levels <- read.csv("http://www.waterdatafortexas.org/reservoirs/recent-conditions.csv")
# writing to aws: 
tmp <- tempfile()
write.csv(res_levels, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_reservoir_levels_recent.csv",
  bucket = "tech-team-data",
)

# grabbing historic reservoir levels: 
tx_res <- c("brazos", "canadian", "colorado", "cypress", "guadalupe", "lavaca", 
            "neches", "nueces", "red", "rio-grande", "sabine", "san-antonio", "san-jacinto", 
            "sulphur", "trinity")
# grabbing historic reservoir levels: 
res_data <- data.frame()
for(i in 1:length(tx_res)){
  res_i <- tx_res[i]
  print(res_i)
  download_link <- paste0("https://www.waterdatafortexas.org/reservoirs/basin/", res_i, ".csv")
  res_data_i <- read.csv(download_link, skip = 29) 
  res_data_i$reservoir <- res_i
  res_data <- rbind(res_data, res_data_i)
}
# filtering for the past 10 years, since some records go back to 1938
res_data$date <- as.Date(res_data$date, tryFormats = "%Y-%m-%d")
res_data$year <- year(res_data$date)
res_data_filtered <- res_data %>%
  filter(year >= 2013)
# writing to aws: 
tmp <- tempfile()
write.csv(res_data_filtered, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_reservoir_levels_10yr.csv",
  bucket = "tech-team-data",
)

## Groundwater levels: https://www.waterdatafortexas.org/groundwater
# downloaded Jan 24 2024 
# NOTE: only recent conditions - update more frequently?
well_levels <- read.csv("https://www.waterdatafortexas.org/groundwater/recent-conditions.csv") %>%
  janitor::clean_names()
# writing to aws: 
tmp <- tempfile()
write.csv(well_levels, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_recent_well_levels.csv",
  bucket = "tech-team-data",
)
```

## Water Delivery System 
```{r}
## Water Delivery System #######################################################
# grabbing SDWIS data - currently looking at all of them by leaving cont_code
# as blank
sdwis <- get_SDWIS(pwsids = pwsids, 
                   cont_code = c())
tmp <- tempfile()
write.csv(sdwis, file = tmp)
on.exit(unlink(tmp))
put_object(
  file = tmp,
  object = "/state-drinking-water/TX/raw/TX_sdwis.csv",
  bucket = "tech-team-data",
)
```

## Financial
```{r}
## TX DAC - whether they are likely to qualify for DAC status - requires pulling 
# water affordability data and combining this with mhi. 
## Water rates: https://raw.githubusercontent.com/NIEPS-Water-Program/water-affordability/main/www/data/all_rates_table_current.csv
# downloaded Jan 31 2024
raw_link <- "https://raw.githubusercontent.com/NIEPS-Water-Program/water-affordability/main/www/data/all_rates_table_current.csv"
temp_file <- tempfile(fileext = ".csv")
water_rates <- GET(raw_link, write_disk(path = temp_file))
water_rates <- read.csv(temp_file) %>%
  janitor::clean_names()

# filtering for just TX data, and household use == 6000 gal/month
tx_rates <- water_rates[grepl("TX", water_rates$pwsid),] %>%
  filter(hh_use == 6000) %>%
  select(c("pwsid", "category", "hh_use", "total_water", "total_sewer")) %>%
  mutate(total_water_year = total_water*12, 
         total_sewer_year = total_sewer*12)

# need to merge this with mhi: 
mhi <- aws.s3::s3read_using(st_read, 
                           object = "state-drinking-water/TX/raw/TX_sab_census.geojson",
                           bucket = "tech-team-data") %>%
  select(c("pwsid", "estimate_mhi"))

# calculating household cost factor: 
tx_rates_hcf <- tx_rates %>%
  left_join(mhi) %>%
  mutate(hcf = (total_water_year + total_sewer_year)/estimate_mhi) %>%
  filter(category == "inside")

# adding to s3: 
tmp <- tempfile()
st_write(tx_rates_hcf, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/raw/TX_DAC.geojson",
  bucket = "tech-team-data",
)
```

## Requires manual download 
```{r}
# Climate Vulnerability Index: https://map.climatevulnerabilityindex.org/map/cvi_overall/usa?mapBoundaries=Tract&mapFilter=0&reportBoundaries=Tract&geoContext=State
# manually downloaded from dropbox on Jan 26 2024 (there is no api) and 
# added to my ./data/raw folder: 
cvi <- readxl::read_excel("./data/raw/MasterCVIDataset_Oct2023.xlsx", 
           sheet = "Domain CVI Values") %>%
  filter(State == "TX") %>%
  janitor::clean_names()

# writing to aws: 
tmp <- tempfile()
write.csv(cvi, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_CVI.csv",
  bucket = "tech-team-data",
)

## Historic use estimates: https://www.twdb.texas.gov/waterplanning/waterusesurvey/estimates/index.asp
# downloaded Jan 24 2024
# Without an api or data download link, I manually downloaded data from 
# 2016 - 2020 from here: https://www3.twdb.texas.gov/apps/reports/WU_REP/PWS_Categorical_Connections_and_Volumes
# and dropped it in my local ./data/raw folder: 
folder <- "./data/raw/TX_historic_use_estimates/"
files <- list.files(folder)
year_use_estimates <- lapply(paste0(folder, files), read.csv, header = TRUE)
historic_use <- do.call(rbind, year_use_estimates) %>%
  janitor::clean_names()

# writing to aws: 
tmp <- tempfile()
write.csv(historic_use, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_historic_use_estimates.csv",
  bucket = "tech-team-data",
)


## Water loss audits: https://www.twdb.texas.gov/conservation/municipal/waterloss/historical-annual-report.asp
# downloaded Jan 24 2024
# This requires merging in downloaded file that is stored in s3 as 
# TX_historic_use_estimates.csv, looping over to cleaning and then merge 


## CDC SVI: https://www.atsdr.cdc.gov/placeandhealth/svi/interactive_map.html
# downloaded Jan 26 2024
# This requires merging in downloaded file that is stored in s3 as 
# TX_SVI_2020.csv


## EJ screen: https://gaftp.epa.gov/EJScreen/2023/2.22_September_UseMe/EJSCREEN_2023_Tracts_StatePct_with_AS_CNMI_GU_VI.csv.zip
# downloaded Feb 13 2024
# dropping this in the manual download section because the csv link will likely 
# change as the data are updated. 
# NOTE: the R package and API are not very user-friendly. 
file_loc <- "./data/raw/ejscreen"
download.file("https://gaftp.epa.gov/EJScreen/2023/2.22_September_UseMe/EJSCREEN_2023_Tracts_StatePct_with_AS_CNMI_GU_VI.csv.zip", destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))

# tidying and querying for TX: 
TX_ejscreen <- read.csv(paste0(file_loc, "/", list.files(file_loc))) %>%
  filter(ST_ABBREV == "TX") %>%
  janitor::clean_names()
write.csv(TX_ejscreen, paste0(file_loc, "/", "tx_ejscreen.csv"))

# writing to aws: 
put_object(
  file = paste0(file_loc, "/", "tx_ejscreen.csv"),
  object = "/state-drinking-water/TX/raw/TX_ejscreen.csv",
  bucket = "tech-team-data"
)
```
