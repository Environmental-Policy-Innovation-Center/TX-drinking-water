---
title: "Texas Drinking Water Data Downloading"
author: "EmmaLi Tsai"
date: "2024-01-29"
last_updated: "2024-07-11; swapping to EPA SABs"
output: html_document
---

Packages: 
```{r packages}
library(tidyverse)
library(leaflet)
library(sf)
library(aws.s3) 
library(janitor)
library(tidycensus)
library(areal)
library(mapview)
library(httr)
library(readxl)
library(jsonlite)
library(googlesheets4)
library(tigris)
options(tigris_use_cache = TRUE)
# remotes::install_github("environmental-policy-innovation-center/stateDW",
#                         auth_token = "insert-github-pat-here")
# library(stateDW)

# Store keys on local machine in .Renviron file! 
```

Functions: 
```{r functions}
# Migrated to R package workflow but keeping these here as a backup 
# source("./code/functions/get_sab.R")
# source("./code/functions/get_SDWIS.R")
# source("./code/functions/interp_sab_census.R")
```

_______________________________________________________________________________
### Data Download - code for web scraping & saving raw files. Data here either take a long time to download, require extra tidying, are updated frequently (i.e., reservoir or aquifer levels) or need to be manually downloaded from a website. Everything in this section is added to S3.
_______________________________________________________________________________
## Demographic
```{r data collection}
# SOP for TX-specific data: ####################################################
# PWSIDs should have "TX" at the beginning of them 
# All data must have either pwsid, aquifer, census id (block group/tract), and 
# well number (or any match-able columns)
# Group data collections by category (demographic, environmental, financial, etc. )
# All data should be in wide format 
# All data should have a pulled link to note where it came from
# Add data to the TX_data list 
# Variable naming convention - janitor::clean_names()

## Demographic #################################################################
# these are the EPA's SABs - everything has been crosswalked in water-data repo
sab <- aws.s3::s3read_using(st_read, 
                           object = "service_area_boundaries/epa-sabs/epa-sabs-crosswalk.geojson",
                           bucket = "tech-team-data") %>%
  filter(crosswalk_state == "TX") %>%
  select(-crosswalk_state)

# adding other SDWIS columns that are not in the new sab dataset that we want: 
sdwis_ws <- aws.s3::s3read_using(read.csv, 
                                 object = "/service_area_boundaries/epa-sabs/sdwis-water-systems.csv",
                                 bucket = "tech-team-data") %>%
  filter(state_code == "TX") %>%
  select(pwsid, primacy_type, primary_source_code, owner_type_code, city_name)

# merging sdwis vars into SAB dataset: 
sab_sdwis <- merge(sab, sdwis_ws, by = "pwsid", all.x = T) %>%
  relocate(primacy_type:city_name, .after = primacy_agency)
# NOTE: we no longer have the east_tx_flag, county_served, or service area type
# I tried to grab most of these - like city_served, and system owner type

# SAB & Census - writing this to s3: 
tmp <- tempfile()
st_write(sab_sdwis, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/raw/TX_EPAsab_census.geojson",
  bucket = "tech-team-data",
)
```

## Environmental
```{r}
## Environmental ###############################################################
## Groundwater database: https://www.twdb.texas.gov/groundwater/data/GWDBDownload.zip
# Pulled Jan 22 2024 
file_loc <- "./data/raw/TX_GWDB"
download.file("https://www.twdb.texas.gov/groundwater/data/GWDBDownload.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
## TODO: add code that downloads & uploads entire folder to aws (HOLD 
# on this since we don't actually use this dataset in the analysis)

## Reservoir levels: https://www.waterdatafortexas.org/reservoirs/statewide
# downloaded Jan 24 2024 
# NOTE: only recent conditions - update more frequently?
res_levels <- read.csv("http://www.waterdatafortexas.org/reservoirs/recent-conditions.csv")
# writing to aws: 
tmp <- tempfile()
write.csv(res_levels, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_reservoir_levels_recent.csv",
  bucket = "tech-team-data",
)

# grabbing historic reservoir levels: 
tx_res <- c("brazos", "canadian", "colorado", "cypress", "guadalupe", "lavaca", 
            "neches", "nueces", "red", "rio-grande", "sabine", "san-antonio", "san-jacinto", 
            "sulphur", "trinity")
# grabbing historic reservoir levels: 
res_data <- data.frame()
for(i in 1:length(tx_res)){
  res_i <- tx_res[i]
  print(res_i)
  download_link <- paste0("https://www.waterdatafortexas.org/reservoirs/basin/", res_i, ".csv")
  res_data_i <- read.csv(download_link, skip = 29) 
  res_data_i$reservoir <- res_i
  res_data <- rbind(res_data, res_data_i)
}
# filtering for the past 10 years, since some records go back to 1938
res_data$date <- as.Date(res_data$date, tryFormats = "%Y-%m-%d")
res_data$year <- year(res_data$date)
res_data_filtered <- res_data %>%
  filter(year >= 2013)
# writing to aws: 
tmp <- tempfile()
write.csv(res_data_filtered, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_reservoir_levels_10yr.csv",
  bucket = "tech-team-data",
)

## Groundwater levels: https://www.waterdatafortexas.org/groundwater
# downloaded Jan 24 2024 
# NOTE: only recent conditions - update more frequently?
well_levels <- read.csv("https://www.waterdatafortexas.org/groundwater/recent-conditions.csv") %>%
  janitor::clean_names()
# writing to aws: 
tmp <- tempfile()
write.csv(well_levels, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_recent_well_levels.csv",
  bucket = "tech-team-data",
)
```

## Water Delivery System 
```{r}
## Water Delivery System #######################################################
# grabbing SDWIS data - currently looking at all of them by leaving cont_code
# as blank
sdwis <- get_SDWIS(pwsids = pwsids, 
                   cont_code = c())
tmp <- tempfile()
write.csv(sdwis, file = tmp)
on.exit(unlink(tmp))
put_object(
  file = tmp,
  object = "/state-drinking-water/TX/raw/TX_sdwis.csv",
  bucket = "tech-team-data",
)
```

## Financial
```{r}
## TX DAC - whether they are likely to qualify for DAC status - requires pulling 
# water affordability data and combining this with mhi. 
## Water rates: https://raw.githubusercontent.com/NIEPS-Water-Program/water-affordability/main/www/data/all_rates_table_current.csv
# downloaded Jan 31 2024
raw_link <- "https://raw.githubusercontent.com/NIEPS-Water-Program/water-affordability/main/www/data/all_rates_table_current.csv"
temp_file <- tempfile(fileext = ".csv")
water_rates <- GET(raw_link, write_disk(path = temp_file))
water_rates <- read.csv(temp_file) %>%
  janitor::clean_names()

# filtering for just TX data, and household use == 6000 gal/month
tx_rates <- water_rates[grepl("TX", water_rates$pwsid),] %>%
  filter(hh_use == 6000) %>%
  select(c("pwsid", "category", "hh_use", "total_water", "total_sewer")) %>%
  mutate(total_water_year = total_water*12, 
         total_sewer_year = total_sewer*12)

# need to merge this with mhi: 
mhi <- aws.s3::s3read_using(st_read, 
                           object = "state-drinking-water/TX/raw/TX_sab_census.geojson",
                           bucket = "tech-team-data") %>%
  select(c("pwsid", "estimate_mhi"))

# calculating household cost factor: 
tx_rates_hcf <- tx_rates %>%
  left_join(mhi) %>%
  mutate(hcf = (total_water_year + total_sewer_year)/estimate_mhi) %>%
  filter(category == "inside")

# adding to s3: 
tmp <- tempfile()
st_write(tx_rates_hcf, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/raw/TX_DAC.geojson",
  bucket = "tech-team-data",
)
```

## Requires manual download 
```{r}
# Climate Vulnerability Index: https://map.climatevulnerabilityindex.org/map/cvi_overall/usa?mapBoundaries=Tract&mapFilter=0&reportBoundaries=Tract&geoContext=State
# manually downloaded from dropbox on Jan 26 2024 (there is no api) and 
# added to my ./data/raw folder: 
cvi <- readxl::read_excel("./data/raw/MasterCVIDataset_Oct2023.xlsx", 
           sheet = "Domain CVI Values") %>%
  filter(State == "TX") %>%
  janitor::clean_names()

# writing to aws: 
tmp <- tempfile()
write.csv(cvi, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_CVI.csv",
  bucket = "tech-team-data",
)

## Historic use estimates: https://www.twdb.texas.gov/waterplanning/waterusesurvey/estimates/index.asp
# downloaded Jan 24 2024
# Without an api or data download link, I manually downloaded data from 
# 2016 - 2020 from here: https://www3.twdb.texas.gov/apps/reports/WU_REP/PWS_Categorical_Connections_and_Volumes
# and dropped it in my local ./data/raw folder: 
folder <- "./data/raw/TX_historic_use_estimates/"
files <- list.files(folder)
year_use_estimates <- lapply(paste0(folder, files), read.csv, header = TRUE)
historic_use <- do.call(rbind, year_use_estimates) %>%
  janitor::clean_names()

# writing to aws: 
tmp <- tempfile()
write.csv(historic_use, file = paste0(tmp, ".csv"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/raw/TX_historic_use_estimates.csv",
  bucket = "tech-team-data",
)


## Water loss audits: https://www.twdb.texas.gov/conservation/municipal/waterloss/historical-annual-report.asp
# downloaded Jan 24 2024
# This requires merging in downloaded file that is stored in s3 as 
# TX_historic_use_estimates.csv, looping over to cleaning and then merge 


## CDC SVI: https://www.atsdr.cdc.gov/placeandhealth/svi/interactive_map.html
# downloaded Jan 26 2024
# This requires merging in downloaded file that is stored in s3 as 
# TX_SVI_2020.csv


## EJ screen: https://gaftp.epa.gov/EJScreen/2023/2.22_September_UseMe/EJSCREEN_2023_Tracts_StatePct_with_AS_CNMI_GU_VI.csv.zip
# downloaded Feb 13 2024
# dropping this in the manual download section because the csv link will likely 
# change as the data are updated. 
# NOTE: the R package and API are not very user-friendly. 
file_loc <- "./data/raw/ejscreen"
download.file("https://gaftp.epa.gov/EJScreen/2023/2.22_September_UseMe/EJSCREEN_2023_Tracts_StatePct_with_AS_CNMI_GU_VI.csv.zip", destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))

# tidying and querying for TX: 
TX_ejscreen <- read.csv(paste0(file_loc, "/", list.files(file_loc))) %>%
  filter(ST_ABBREV == "TX") %>%
  janitor::clean_names()
write.csv(TX_ejscreen, paste0(file_loc, "/", "tx_ejscreen.csv"))

# writing to aws: 
put_object(
  file = paste0(file_loc, "/", "tx_ejscreen.csv"),
  object = "/state-drinking-water/TX/raw/TX_ejscreen.csv",
  bucket = "tech-team-data"
)
```
