---
title: "TX Water Data Cleaning & Processing"
output: html_document
date: "2023-12-07"
author: "EmmaLi Tsai" 
---

Packages: 
```{r}
library(tidyverse)
library(leaflet)
library(sf)
library(aws.s3) 
library(janitor)
library(tidycensus)
library(areal)
library(mapview)
library(httr)
library(readxl)
library(jsonlite)
library(googlesheets4)
# remotes::install_github("environmental-policy-innovation-center/stateDW", 
#                         auth_token = "insert-github-pat-here")
library(stateDW)

# Store keys on local machine in .Renviron file! 
```

Sourcing Downloaders: 
```{r}
# Migrated to R package workflow but keeping these here as a backup 
# source("./code/functions/get_sab.R")
# source("./code/functions/get_SDWIS.R")
# source("./code/functions/interp_sab_census.R")
```

Grabbing Data Nationally Available from APIs: 
```{r}
# grabbing sabs: 
sab <- get_sab(states = c("TX"), crs = ("ESRI:102296"))
# making note the of the pwsids located in TX: 
pwsids <- unique(sab$pwsid)

# grabbing SDWIS data - currently looking at all of them by leaving cont_code
# as blank
sdwis <- get_SDWIS(pwsids = pwsids, 
                   cont_code = c())

# census data - grabbing block group stats for now 
# vars_acs <- load_variables(year = 2021, "acs5")
census_vars <- c(total_pop = "B01003_001",
                 black_alone = "B02001_003", 
                 asian_alone = "B02001_005", 
                 white_alone = "B02001_002", 
                 AIAN_alone = "B02001_004", 
                 NAPI_alone = "B02001_006", 
                 other_alone = "B02001_007", 
                 mixed_alone = "B02001_008",
                 hisp_alone = "B03003_003", 
                 mhi = "B19013_001", 
                 under_6  = "B23008_002", 
                 over_65 = "B09021_022", 
                 only_english = "B99162_002", 
                 other_lang = "B99162_003", 
                 no_school = "B15003_002", 
                 prof_degree = "B15003_024", 
                 foreign = "B99051_005")
census <- tidycensus::get_acs(
  geography = "block group", 
  variables = census_vars, 
  state = unique(sab$state_code), 
  year = 2021,
  geometry = TRUE
)
```

Merging & Organizing Datasets Available Nationally: 
```{r}
## Calculating census stats for pwsids: ########################################
# NOTE: this is the areal interpolation method with weights = "total" 
sab_census <- interp_sab_census(census_df = census, sab_df = sab, 
                                total_pop_var = "estimate_total_pop", 
                                white_alone_var = "estimate_white_alone")

# Reading & Writing to buckets 
# SDWIS:
# tmp <- tempfile()
# write.csv(sdwis, file = tmp)
# on.exit(unlink(tmp))
# put_object(
#   file = tmp,
#   object = "/state-drinking-water/TX/raw/TX_sdwis.csv",
#   bucket = "tech-team-data",
# )

# SAB & Census:
# tmp <- tempfile()
# st_write(sab_census, dsn = paste0(tmp, ".geojson"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".geojson"),
#   object = "/state-drinking-water/TX/raw/TX_sab_census.geojson",
#   bucket = "tech-team-data",
# )
```

Maps & Plots (QA/QC) for national data that has been queried to TX 
```{r}
## QA/QC #####################################################################
# grabbing summary stats for the census and sdwis violations:  
sdwis_viols <- sdwis_tidy %>%
  group_by(pwsid) %>%
  summarize(total_viol = length(unique(violation_id)))

# grabbing summary stats for the census and sdwis violations:  
sab_sdwis_summary <- sab_census %>%
  group_by(pwsid, primary_source_code) %>%
  mutate(population_served_count = as.numeric(population_served_count)) %>%
  summarize(service_connections = mean(service_connections_count),
            sdwis_total_pop = mean(population_served_count), 
            area = mean(area_miles), 
            pop_density = mean(pop_density), 
            census_total_pop = mean(estimate_total_pop),
            mean_POC_per = mean(estimate_poc_alone_per)) %>%
  left_join(., sdwis_viols)
st_write(sab_sdwis_summary, dsn = "./data/raw/sab_sdwis_summary.geojson")

# quick stats to get an idea of what's happening at a glance: 
# TODO: add number of sabs outside of TX boundary 
diag_log <- data.frame(diagnostic = "") %>%
  mutate(na_pwsid = sum(is.na(sab_sdwis_summary$pwsid)), 
         na_serv_connections = sum(is.na(sab_sdwis_summary$service_connections)),
         na_sdwis_pop = sum(is.na(sab_sdwis_summary$sdwis_total_pop)), 
         zero_sdwis_pop = sum(sab_sdwis_summary$sdwis_total_pop == 0, 
                              na.rm = TRUE), 
         zero_area = sum(sab_sdwis_summary$area == 0,
                         na.rm = TRUE), 
         zero_pop_density = sum(sab_sdwis_summary$pop_density %in% 
                                  c(0, "NaN", "Inf"),
                                na.rm = TRUE), 
         na_cesus_pop = sum(is.na(sab_sdwis_summary$census_total_pop)), 
         zero_census_pop = sum(round(sab_sdwis_summary$census_total_pop, 0) %in% 
                                 c(0, "NaN", "Inf"), 
                               na.rm = TRUE), 
         empty_geoms = sum(sf::st_is_empty(sab_sdwis_summary))) %>%
  select(-diagnostic) %>%
  pivot_longer(., cols = everything())
# moving this to results
write.csv(diag_log, "./results/qa_qc/diagnostic_log.csv")

## Map of pwsids and % poc ####################################################
# converting to sf, transformations, and filtering 
sab_census_sf <- sab_census %>%
  st_as_sf(.) %>%
  st_transform(., '+proj=longlat +datum=WGS84') %>% 
  filter(!st_is_empty(.))

poc_pal <- colorNumeric(
  palette = viridis::turbo(6),
  domain = sab_census_sf$estimate_poc_alone_per)
poc_map <- leaflet() %>%
  addProviderTiles(providers$CartoDB.VoyagerNoLabels, group = "Toner Lite")  %>%
  addPolygons(data = sab_census_sf, 
              opacity = 9,
              color = ~poc_pal(estimate_poc_alone_per),
              weight = 1, 
              highlightOptions = highlightOptions(color = "grey", weight = 2,
                                                  bringToFront = TRUE), 
              label = paste0("pwsid: ", sab_census_sf$pwsid, 
                             " / poc %: ", 
                             round(sab_census_sf$estimate_poc_alone_per, 0))) %>%
    addLegend("bottomright", 
            pal = poc_pal, 
            values = sab_census_sf$estimate_poc_alone_per,
            title = "% POC",
            opacity = 1)
mapview::mapshot2(poc_map, file = "./results/qa_qc/pwsid_poc_map.png")

## Map of pwsids and # SDWIS violations ########################################
# finding the total number of violations (need to also standardize for year and
# how long the system has been operating for):
# violations <- sdwis_tidy %>%
#   group_by(pwsid) %>%
#   summarize(total_viol = length(unique(violation_id)))
# grabbing geoms for mapping: 
violations_geoms <- merge(sab[,c("pwsid")], sdwis_viols, all = T) %>%
  st_as_sf(.) %>%
  st_transform(., '+proj=longlat +datum=WGS84') %>% 
  filter(!st_is_empty(.))
  
# map of total number of violations for pwsid: 
viol_pal <- colorNumeric(
  palette = viridis::plasma(6),
  domain = violations_geoms$total_viol)
viol_map <- leaflet() %>%
  addProviderTiles(providers$CartoDB.VoyagerNoLabels, group = "Toner Lite")  %>%
  addPolygons(data = violations_geoms, 
              opacity = 9,
              color = ~viol_pal(total_viol),
              weight = 1, 
              highlightOptions = highlightOptions(color = "grey", weight = 2,
                                                  bringToFront = TRUE), 
              label = paste0("pwsid: ", violations_geoms$pwsid, 
                             " / violations: ", violations_geoms$total_viol)) %>%
    addLegend("bottomright", 
            pal = viol_pal, 
            values = violations_geoms$total_viol,
            title = "# violations",
            opacity = 1)
mapview::mapshot2(viol_map, file = "./results/qa_qc/pwsid_violation_map.png")

# SDWIS violations by % POC: 
poc_violations <- ggplot(sab_sdwis_summary, 
       aes(x = mean_POC_per, y = total_viol, color = primary_source_code)) +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none") + 
  facet_wrap(~primary_source_code, ncol = 1, scales = "free") + 
  labs(x = "Percent Person of Color", y = "Number of Violations")
ggsave("./results/qa_qc/poc_violation_plot.png", poc_violations)

pop_violations <- ggplot(sab_sdwis_summary, 
                         aes(x = sdwis_total_pop, y = total_viol, 
                             color = total_viol)) +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "Total Population", y = "Number of Violations")
ggsave("./results/qa_qc/pop_violation_plot.png", pop_violations)
```

_______________________________________________________________________________
Start of TX-specific data collection code: 
_______________________________________________________________________________
Merging & Organizing Datasets Available Only in TX
```{r}
# SOP for TX-specific data: ##################
# PWSIDs should have "TX" at the beginning of them 
# All data must have either pwsid, aquifer, census id (block group/tract), and well number (or any match-able 
# columns)
# Group data collections by category (demographic, environmental, financial, etc. )
# All data should be in wide format 
# All data should have a pulled link to note where it came from
# Add data to the TX_data list 
# Variable naming convention - janitor::clean_names()
```

Environmental: 
```{r}
## Surface water intake locations: https://gis-tceq.opendata.arcgis.com/datasets/TCEQ::surface-intakes/api
# Pulled Jan 22 2024 
sw_intake <- GET("https://gisweb.tceq.texas.gov/arcgis/rest/services/Public/PWS/MapServer/1/query?where=1%3D1&outFields=*&outSR=4326&f=json")
sw_intake_tidy <- fromJSON(rawToChar(sw_intake$content))$features$attribute %>%
  as.data.frame() %>%
  janitor::clean_names() %>%
  rename(pwsid = pws_id) %>%
  mutate(pwsid = paste0("TX", pwsid))
# writing to aws: 
# tmp <- tempfile()
# write.csv(sw_intake_tidy, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_sw_intake.csv",
#   bucket = "tech-team-data",
# )

## Groundwater database: https://www.twdb.texas.gov/groundwater/data/GWDBDownload.zip
# Pulled Jan 22 2024 
file_loc <- "./data/raw/TX_GWDB"
download.file("https://www.twdb.texas.gov/groundwater/data/GWDBDownload.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
## TODO: add code that downloads & uploads entire folder to aws
# put_folder(
#   folder = "./data/raw/TX_GWDB/GWDBDownload/",
#   bucket = "s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB_test"
# )

## Major aquifers: https://www.twdb.texas.gov/mapping/gisdata/doc/major_aquifers.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_major_aquifers"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/major_aquifers.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
maj_aquifers <- st_read(paste0(file_loc, "/NEW_major_aquifers_dd.shp")) %>%
  janitor::clean_names()
# writing to aws: 
# tmp <- tempfile()
# st_write(maj_aquifers, dsn = paste0(tmp, ".geojson"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".geojson"),
#   object = "/state-drinking-water/TX/raw/major_aquifers.geojson",
#   bucket = "tech-team-data",
# )

## Minor aquifers: https://www.twdb.texas.gov/mapping/gisdata/doc/minor_aquifers.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_minor_aquifers"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/minor_aquifers.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
minor_aquifers <- st_read(paste0(file_loc, "/Minor_Aquifers.shp")) %>%
  janitor::clean_names()
# tmp <- tempfile()
# st_write(minor_aquifers, dsn = paste0(tmp, ".geojson"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".geojson"),
#   object = "/state-drinking-water/TX/raw/TX_minor_aquifers.geojson",
#   bucket = "tech-team-data",
# )

## Major river basins: https://www.twdb.texas.gov/mapping/gisdata/doc/Major_River_Basins_Shapefile.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_major_river_basins"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/Major_River_Basins_Shapefile.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
major_river_basins <- st_read(paste0(file_loc, "/TWDB_MRBs_2014.shp")) %>%
  janitor::clean_names()
# tmp <- tempfile()
# st_write(major_river_basins, dsn = paste0(tmp, ".geojson"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".geojson"),
#   object = "/state-drinking-water/TX/raw/major_river_basins.geojson",
#   bucket = "tech-team-data",
# )

## Existing reservoirs: https://www.twdb.texas.gov/mapping/gisdata/doc/Existing_Reservoirs.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_reservoirs"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/Existing_Reservoirs.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
reservoirs <- st_read(paste0(file_loc, "/TWDB_SWP2012_Major_Reservoirs.shp")) %>%
  janitor::clean_names()
# tmp <- tempfile()
# st_write(reservoirs, dsn = paste0(tmp, ".geojson"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".geojson"),
#   object = "/state-drinking-water/TX/raw/reservoirs.geojson",
#   bucket = "tech-team-data",
# )

## Surface water 303(d) river basin impairment: https://texaswaterexplorer.tnc.org/index.html
# downloaded 1/24/2024
file_loc <- "./data/raw/TX_basin_impairment_2012"
download.file("https://texaswaterexplorer.tnc.org/downloads/Data_WQImpairment.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
basin_impairment <- readxl::read_excel(paste0(file_loc, "/Data_WQImpairment.xlsx"), sheet = "Basins")%>%
  janitor::clean_names()
# tmp <- tempfile()
# write.csv(basin_impairment, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/basin_impairment.csv",
#   bucket = "tech-team-data",
# )

## Reservoir levels: https://www.waterdatafortexas.org/reservoirs/statewide
# downloaded Jan 24 2024 
# only recent conditions: 
res_levels <- read.csv("http://www.waterdatafortexas.org/reservoirs/recent-conditions.csv")
# writing to aws: 
# tmp <- tempfile()
# write.csv(res_levels, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_reservoir_levels_recent.csv",
#   bucket = "tech-team-data",
# )

# grabbing historic reservoir levels: 
tx_res <- c("brazos", "canadian", "colorado", "cypress", "guadalupe", "lavaca", 
            "neches", "nueces", "red", "rio-grande", "sabine", "san-antonio", "san-jacinto", 
            "sulphur", "trinity")
# grabbing historic reservoir levels: 
res_data <- data.frame()
for(i in 1:length(tx_res)){
  res_i <- tx_res[i]
  print(res_i)
  download_link <- paste0("https://www.waterdatafortexas.org/reservoirs/basin/", res_i, ".csv")
  res_data_i <- read.csv(download_link, skip = 29) 
  res_data_i$reservoir <- res_i
  res_data <- rbind(res_data, res_data_i)
}
# filtering for the past 10 years, since some records go back to 1938
res_data$date <- as.Date(res_data$date, tryFormats = "%Y-%m-%d")
res_data$year <- year(res_data$date)
res_data_filtered <- res_data %>%
  filter(year >= 2013)
# writing to aws: 
# tmp <- tempfile()
# write.csv(res_data_filtered, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_reservoir_levels_10yr.csv",
#   bucket = "tech-team-data",
# )

## Groundwater levels: https://www.waterdatafortexas.org/groundwater
# downloaded Jan 24 2024 
# only recent conditions: 
well_levels <- read.csv("https://www.waterdatafortexas.org/groundwater/recent-conditions.csv") %>%
  janitor::clean_names()
# writing to aws: 
# tmp <- tempfile()
# write.csv(well_levels, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_recent_well_levels.csv",
#   bucket = "tech-team-data",
# )
# grabbing historic data for wells is difficult because they have
# different loggers and therefore different column names (i.e., some have temp 
# and others don't) - decided to pull historic well data as needed. 

# Climate Vulnerability Index: https://map.climatevulnerabilityindex.org/map/cvi_overall/usa?mapBoundaries=Tract&mapFilter=0&reportBoundaries=Tract&geoContext=State
# manually downloaded from dropbox on Jan 26 2024 (there is no api) and 
# added to my ./data/raw folder: 
cvi <- readxl::read_excel("./data/raw/MasterCVIDataset_Oct2023.xlsx", 
           sheet = "Domain CVI Values") %>%
  filter(State == "TX") %>%
  janitor::clean_names()
# write.csv(cvi, "./data/raw/TX_CVI.csv")
# just a note that this other sheet exists which contains ~180 other stats 
# (some are health-related) for each census tract: 
# indicator_native_units <- readxl::read_excel("./data/raw/MasterCVIDataset_Oct2023.xlsx", 
#            sheet = "Indicator Native Units") %>%
#   filter(State == "TX") %>%
#   janitor::clean_names()
```

Water Delivery System
```{r}
## Historic use estimates: https://www.twdb.texas.gov/waterplanning/waterusesurvey/estimates/index.asp
# downloaded Jan 24 2024
# Without an api or data download link, I manually downloaded data from 
# 2016 - 2020 from here: https://www3.twdb.texas.gov/apps/reports/WU_REP/PWS_Categorical_Connections_and_Volumes
# and dropped it in my local ./data/raw folder: 
folder <- "./data/raw/TX_historic_use_estimates/"
files <- list.files(folder)
year_use_estimates <- lapply(paste0(folder, files), read.csv, header = TRUE)
historic_use <- do.call(rbind, year_use_estimates) %>%
  janitor::clean_names()
# writing to aws: 
# tmp <- tempfile()
# write.csv(historic_use, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_historic_use_estimates.csv",
#   bucket = "tech-team-data",
# )

## Water loss audits: https://www.twdb.texas.gov/conservation/municipal/waterloss/historical-annual-report.asp
# downloaded Jan 24 2024
## This requires merging in downloaded file, looping over to cleaning and then merge - 
# code added below when loading all of the data


## 2022 TX State Water Plan: https://2022.texasstatewaterplan.org/statewide?
# downloaded Jan 29 2024
pop <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/population") %>%
  janitor::clean_names()
demands <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/demands") %>%
  janitor::clean_names()
supp <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/supplies") %>%
  janitor::clean_names()
needs <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/needs") %>%
  janitor::clean_names()
strat <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/strategies") %>%
  janitor::clean_names()
# writing to aws: 
# tmp <- tempfile()
# write.csv(strat, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_2022_SWP/strat.csv",
#   bucket = "tech-team-data",
# )
```

Financial
```{r}
## Water affordability: https://github.com/NIEPS-Water-Program/water-affordability/raw/main/data/rates_data/rates_tx.xlsx
# downloaded Jan 24 2024
link <- "https://github.com/NIEPS-Water-Program/water-affordability/raw/main/data/rates_data/rates_tx.xlsx"
temp_file <- tempfile(fileext = ".xlsx")
water_aff <- GET(github_link, write_disk(path = temp_file))
water_aff <- readxl::read_excel(temp_file) %>%
  janitor::clean_names()
# writing to aws: 
# tmp <- tempfile()
# write.csv(water_aff, file = paste0(tmp, ".csv"))
# on.exit(unlink(tmp))
# put_object(
#   file = paste0(tmp, ".csv"),
#   object = "/state-drinking-water/TX/raw/TX_water_affordability.csv",
#   bucket = "tech-team-data",
# )

## Danielle's SRF analysis: 
gs4_deauth()
URL <- "https://docs.google.com/spreadsheets/d/1kUHLdzFjwmKx4r6YTu2qjxwNuuG5JeUi/edit#gid=1618705547"
non_applying_cities <- read_sheet(URL, sheet = "Analysis")
```

East TX 
```{r}
# TX counties in East Texas (done with some webpage scraping): 
east_TX <- read.csv("./data/raw/east_TX_counties.csv") %>%
  select(-X) %>%
  janitor::clean_names()
```

_______________________________________________________________________________
Loading all data for TX:
_______________________________________________________________________________
```{r}
## National datasets: ##########################################################
# Reading SDWIS, SABs x Census, and CEJST from buckets: 
sdwis_tidy <- aws.s3::s3read_using(read.csv, 
                           object = "state-drinking-water/TX/raw/TX_sdwis.csv", 
                           bucket = "tech-team-data")
sab_census <- aws.s3::s3read_using(st_read, 
                           object = "state-drinking-water/TX/raw/TX_sab_census.geojson",
                           bucket = "tech-team-data")
## CEJST v1.0: 
## Pulled Jan 31 2023:s3://tech-team-data/ej-data/cejest-v1/1.0-communities.csv
## Note using file stored in tech-team-data/ej-data to preserve single conical data practice (SCDP)
cejst_raw <- aws.s3::s3read_using(read.csv, object = "s3://tech-team-data/ej-data/cejest-v1/1.0-communities.csv")

## Selecting variables we need (different then block level acs estimate data) ## 
## census_tract_2010_id
## total_threshold_criteria_exceeded
## total_categories_exceeded 
## identified_as_disadvantaged
## percentage_of_tract_that_is_disadvantaged_by_area
## total_population 
cejst_cleaned <- cejst_raw %>%
  janitor::clean_names() %>%
  # added filter for TX data: 
  filter(state_territory == "Texas") %>%
  select(c("census_tract_2010_id","total_threshold_criteria_exceeded",
           "total_categories_exceeded","identified_as_disadvantaged",
           "percentage_of_tract_that_is_disadvantaged_by_area","total_population"))%>%
  rename("total_population_cejst" = "total_population")

# Social Vulnerability Index 
svi <- aws.s3::s3read_using(read.csv, 
                            object = "s3://tech-team-data/ej-data/cejest-v1/1.0-communities.csv")

# Climate Vulnerability Index: 
cvi <- aws.s3::s3read_using(read.csv, 
                            object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_CVI.csv")

## TX-specific datasets: #######################################################
## Environmental: 
# surface water intake locations: 
sw_intake <- aws.s3::s3read_using(read.csv, 
                               object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_SVI_2020.csv") 

# groundwater quality & levels: 
# function for reading gwdb tables, which have unique separators: 
read_gwdb_table <- function(x) {
     read.table(x, header = TRUE, fill = TRUE, sep = "|")
}
# grabbing well metadata: 
well_main <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WellMain.txt") %>%
  janitor::clean_names()

# water quality information: 
wq_major <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterQualityMajor.txt") %>%
  janitor::clean_names()
wq_minor <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterQualityMinor.txt") %>%
  janitor::clean_names()
wq_other <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterQualityOtherUnassigned.txt") %>%
  janitor::clean_names()
# let's just combine all of these 
well_wq <- rbind(wq_major, wq_minor, wq_other)

# water level information: 
wl_major <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterLevelsMajor.txt") %>%
  janitor::clean_names()
wl_minor <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterLevelsMinor.txt") %>%
  janitor::clean_names()
wl_other <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterLevelsOtherUnassigned.txt") %>%
  janitor::clean_names()
# let's just combine all of these 
well_wl <- rbind(wl_major, wl_minor, wl_other)

# major aquifers
major_aquifers <- aws.s3::s3read_using(st_read, 
                                       object = "s3://tech-team-data/state-drinking-water/TX/raw/major_aquifers.geojson")
# minor aquifers 
minor_aquifers <- aws.s3::s3read_using(st_read, 
                                       object = "s3://tech-team-data/state-drinking-water/TX/raw/minor_aquifers.geojson")
# major river basins
major_river_basins <- aws.s3::s3read_using(st_read, 
                                       object = "s3://tech-team-data/state-drinking-water/TX/raw/major_river_basins.geojson")
# existing reservoirs
reservoirs <- aws.s3::s3read_using(st_read, 
                                       object = "s3://tech-team-data/state-drinking-water/TX/raw/reservoirs.geojson")
# surface water river basin impairment
basin_impairment <- aws.s3::s3read_using(read.csv, 
                                         object = "s3://tech-team-data/state-drinking-water/TX/raw/basin_impairment.csv")
# reservoir levels - recent
recent_reservoir_levels <- aws.s3::s3read_using(read.csv, 
                                         object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_reservoir_levels_recent.csv")

# reservoir levels - historic
reservoir_levels_10yr <- aws.s3::s3read_using(read.csv, 
                                         object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_reservoir_levels_10yr.csv")

# groundwater levels
recent_well_levels <- aws.s3::s3read_using(read.csv, 
                                           object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_recent_well_levels.csv")

# TCEQ DWW system data: 
# NOTE: there is A TON more information in s3 for us to pull like enforcement 
# actions, etc. but starting the structure here for grabbing that information
# pulling data from two different webscraping events and locating pwsids that 
# are community water systems: 
water_details_dec17 <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231217/WaterSystemDetail/TX_Water System Details_20231217_20231229203427.csv") 
water_details_dec3rd <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231203/WaterSystemDetail/TX_Water System Details_20231203.csv") 
combined_details <- rbind(water_details_dec17, water_details_dec3rd)
water_details <- pivot_wider(combined_details, names_from = "Type", values_from = "Value") %>%
  janitor::clean_names()
# grabbing just community water systems (tceq has 10k that are not cws): 
tceq_cws <- water_details %>%
  filter(system_type == "C") # 4666

# pulling violations: 
viol_dec17 <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231217/Violations/TX_Individual Violations_20231217_20231229203427.csv") 
viol_dec3rd <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231203/Violations/TX_Individual Violations_20231203.csv") 
comined_viols <- rbind(viol_dec17, viol_dec3rd) %>%
  janitor::clean_names() %>%
  filter(water_system_no %in% tceq_cws$water_system_no) # 118,659
# SDWIS contains 20,000 more violations  

## Water Delivery System 
# historic use estimates: 
historic_use_estimates <-  aws.s3::s3read_using(read.csv, 
                                                object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_historic_use_estimates.csv")

# water loss audits
# vector of cleaned column names 
waterlosscolnames <- c("Name of Utility",
                       "Real Loss GMD (<32 conn/mi)",
                       "Real Loss GCD",
                       "Apparent Loss GCD",
                       "Water Loss GCD",
                       "ILI (>= 3,000 connections)", 
                       "Total GPCD", 
                       "GPCD Loss",
                       "Real Loss Cost in dollars",
                       "Apparent Loss Cost in dollars","Year")

#making empty dataframe 
waterloss_all <- setNames(data.frame(matrix(ncol = length(waterlosscolnames), nrow = 0)), waterlosscolnames)

year <- 2010 

# ! edit this when adding additional years 
stopyear <- 2022 

while(year <= stopyear)
{
print(year)
waterloss_raw <- aws.s3::s3read_using(read_csv, 
                               object = paste0("s3://tech-team-data/state-drinking-water/TX/raw/TCEQ_Loss_Audits/SummaryAuditsByCategory_",year,".csv"))

waterloss <- waterloss_raw %>%
             select(1:10)

colnames(waterloss) <- waterlosscolnames

waterloss <- waterloss %>%
             janitor::clean_names()%>%
             filter(name_of_utility != "UtilityName")%>%
             mutate(Year = year)

waterloss_all <- rbind(waterloss_all,waterloss)
year <- year + 1
}

# Texas water state plan: 
TX_swp_pop <- aws.s3::s3read_using(read.csv,
                                   object =  "s3://tech-team-data/state-drinking-water/TX/raw/TX_2022_SWP/population.csv")
TX_swp_demands <- aws.s3::s3read_using(read.csv,
                                   object =  "s3://tech-team-data/state-drinking-water/TX/raw/TX_2022_SWP/demands.csv")
TX_swp_supp <- aws.s3::s3read_using(read.csv,
                                   object =  "s3://tech-team-data/state-drinking-water/TX/raw/TX_2022_SWP/supplies.csv")
TX_swp_needs <- aws.s3::s3read_using(read.csv,
                                   object =  "s3://tech-team-data/state-drinking-water/TX/raw/TX_2022_SWP/needs.csv")
TX_swp_strategies <- aws.s3::s3read_using(read.csv,
                                   object =  "s3://tech-team-data/state-drinking-water/TX/raw/TX_2022_SWP/strategies.csv")


## Financial 
# PPL data & tidying 
TX_PPL <- aws.s3::s3read_using(read.csv, 
                               object = "s3://water-team-data/clean_data/srf_project_priority_lists/web_ppl_combined_clean_v1-1.csv") %>%
  mutate(across('Project.Type', 
                str_replace, 'Other', 'General')) %>%
  filter(State == "Texas") %>%
  janitor::clean_names()

# water affordability: 
water_aff <- aws.s3::s3read_using(read.csv, 
                               object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_water_affordability.csv") 
```

_______________________________________________________________________________
Organizing data into a list:
_______________________________________________________________________________
```{r}
# TX_data <- list()
# TX_data[[1]] <- sab_census
# TX_data[[2]] <- sdwis_tidy
# names(TX_data) <- c("census", "sdwis")
# ... 
# # here's how you get the list entry out and into a data frame: 
# sdwis <- do.call(data.frame, TX_data[[2]])
```