---
title: "TX Water Data Cleaning & Processing"
output: html_document
date: "2023-12-07"
author: "EmmaLi Tsai" 
---

Packages: 
```{r}
library(tidyverse)
library(leaflet)
library(sf)
library(aws.s3)
library(janitor)
library(tidycensus)
library(areal)
library(mapview)
```

Sourcing downloaders: 
```{r}
source("~/TX-drinking-water/code/download_handlers/download_sabcrosswalk.R")
source("~/TX-drinking-water/code/download_handlers/download_SDWIS.R")
```

Grabbing data from APIs: 
```{r}
# grabbing sabs: 
sab <- download_sab(states = c("TX"))
# making note the of the pwsids located in TX: 
pwsids <- unique(sab$pwsid)

# grabbing SDWIS data - currently looking at Fluoride and Chlorine violations 
# (1025 and 0999 respectively)
sdwis <- download_SDWIS(pwsids = pwsids, 
                        cont_code = c("1025", "0999"))

# census data - grabbing block group stats for now 
# vars_acs <- load_variables(year = 2021, "acs5")
census_vars <- c(total_pop = "B01003_001",
                black_alone = "B02001_003", 
                asian_alone = "B02001_005", 
                white_alone = "B02001_002", 
                AIAN_alone = "B02001_004", 
                NAPI_alone = "B02001_006", 
                other_alone = "B02001_007", 
                mixed_alone = "B02001_008",
                hisp_alone = "B03003_003", 
                mhi = "B19013_001", 
                under_6  = "B23008_002", 
                over_65 = "B09021_022", 
                only_english = "B99162_002", 
                other_lang = "B99162_003", 
                no_school = "B15003_002", 
                prof_degree = "B15003_024", 
                foreign = "B99051_005")
census <- tidycensus::get_acs(
  geography = "block group", 
  variables = census_vars, 
  state = unique(sab$state_code), 
  year = 2021,
  geometry = TRUE
)
```

Data merging & crosswalking: 
```{r}
## Calculating census stats for pwsids: ########################################
# TODO: Flag for rural or non-rural here? 

# adding counties
census_tidy <- census
counties <- unlist(strsplit(census_tidy$NAME, split = ","))
census_tidy$counties <- trimws(counties[grepl("County", counties)])

# tidying census data and crs transformation: 
census_wide <- pivot_wider(census_tidy, 
                           names_from = c("variable"), 
                           values_from = c("estimate", "moe"))  %>%
  st_transform(crs="ESRI:102296")
# sab crs transformation: 
sab_tidy <- sab %>%
  st_transform(crs="ESRI:102296")

# sab and block group areal interpolation - note this takes quite a bit of time 
# to run
# TODO: brainstorm how to recalculate moe and mhi.. summing them is wrong. 
# Intensive interpolation also isn't right because they're not density or 
# % values... this pub notes that this method for MHI doesn't exist yet
# https://www.mdpi.com/2220-9964/8/7/302
census_vars <- names(census_wide)
ext_varList <- census_vars[grepl("estimate", census_vars)]
areal::ar_validate(source = census_wide,
                   target = sab_tidy,
                   varList = ext_varList,
                   method="aw",
                   verbose=TRUE)
# using the weight = "total" might be more appropriate, since we don't want to 
# allocate EVERY individual within a census tract into a pwsid 
sab_bg_ext <- areal::aw_interpolate(sab_tidy, tid = "pwsid", 
                                    source = census_wide, 
                                    sid = "GEOID", 
                                    weight = "total", 
                                    output = "sf", 
                                    extensive = ext_varList)
# calculating percentage across census vars: 
sab_census_per <- sab_bg_ext %>%
  as.data.frame(.) %>%
  select(starts_with(c("estimate", "pwsid"))) %>%
  select(-c("estimate_mhi")) %>%
  mutate_at(vars(!c("estimate_total_pop", "pwsid")), ~((./estimate_total_pop)*100)) %>%
  relocate("pwsid") %>%
  mutate(estimate_poc_alone = (100 - estimate_white_alone))
# renaming columns for merging purposes: 
colnames(sab_census_per) <- paste0(colnames(sab_census_per), "_per")

# merging this back into the original interpolation: 
sab_census <- merge(sab_bg_ext, sab_census_per, 
      by.x = "pwsid", by.y = "pwsid_per")

## Merging SDWIS, sabs, census ################################################# 
# tidying census data for the year, decade, and duration of violation
sdwis_tidy <- sdwis
sdwis_tidy <- sdwis_tidy %>%
  mutate(across(compl_per_begin_date:compl_per_end_date, 
                ~ as.Date(.x, tryFormats = c("%Y-%m-%d"))), 
         viol_year = year(compl_per_begin_date), 
         viol_decade = viol_year - viol_year %% 10,
         viol_dur = compl_per_end_date - compl_per_begin_date)

# merging it all together: 
census_sdwis <- merge(sab_census, sdwis_tidy, all = T)
## NOTE: this could then write to the data lake using the aws functions 
# contains pwsid-level demographic information and SDWIS violations
# saving locally for now 
write.csv(census_sdwis, "./data/national_raw/pwsid_census_sdwis.csv")
```

Maps & Plots (QA/QC)
```{r}
## QA/QC #####################################################################
# creating a diagnostic log: 
diag_log <- data.frame(diagnostic = "") %>%
  mutate(na_pwsid = sum(is.na(census_sdwis$pwsid)), 
         na_serv_connections = sum(is.na(census_sdwis$service_connections_count)),
         na_sdwis_pop = sum(is.na(census_sdwis$population_served_count)), 
         zero_sdwis_pop = sum(census_sdwis$population_served_count == 0, 
                              na.rm = TRUE), 
         zero_area = sum(census_sdwis$area_miles == 0,
                         na.rm = TRUE), 
         zero_pop_density = sum(census_sdwis$pop_density %in% 
                                  c(0, "NaN", "Inf"),
                                na.rm = TRUE), 
         na_cesus_pop = sum(is.na(census_sdwis$estimate_total_pop)), 
         zero_census_pop = sum(round(census_sdwis$estimate_total_pop, 0) %in% 
                                 c(0, "NaN", "Inf"), 
                               na.rm = TRUE), 
         empty_geoms = sum(sf::st_is_empty(census_sdwis))) %>%
  select(-diagnostic) %>%
  pivot_longer(., cols = everything())
# moving this to results
write.csv(diag_log, "./results/diagnostic_log.csv")

# how does our pwsids compare to other websites? 
# 4552 sabs based on: https://www3.twdb.texas.gov/apps/waterserviceboundaries
# length(unique(TX_sab$pwsid))  # 4586 but the website above might've filtered 
# data that have at least 15 service connections or serve at least 
# 25 people at least 60 days out of the year 

## Map of pwsids and % poc ####################################################
# converting to sf, transformations, and filtering 
sab_census_sf <- sab_census %>%
  st_as_sf(.) %>%
  st_transform(., '+proj=longlat +datum=WGS84') %>% 
  filter(!st_is_empty(.))

poc_pal <- colorNumeric(
  palette = viridis::turbo(6),
  domain = sab_census_sf$estimate_poc_alone_per)

poc_map <- leaflet() %>%
  addProviderTiles(providers$CartoDB.VoyagerNoLabels, group = "Toner Lite")  %>%
  addPolygons(data = sab_census_sf, 
              opacity = 9,
              color = ~poc_pal(estimate_poc_alone_per),
              weight = 1, 
              highlightOptions = highlightOptions(color = "grey", weight = 2,
                                                  bringToFront = TRUE), 
              label = paste0("pwsid: ", sab_census_sf$pwsid, 
                             " / poc %: ", 
                             sab_census_sf$estimate_poc_alone_per)) %>%
    addLegend("bottomright", 
            pal = poc_pal, 
            values = sab_census_sf$estimate_poc_alone_per,
            title = "% POC",
            opacity = 1)
mapview::mapshot2(poc_map, file = "./results/pwsid_poc_map.png")

## Map of pwsids and # SDWIS violations ########################################
# finding the total number of violations (need to also standardize for year and
# how long the system has been operating for):
violations <- sdwis_tidy %>%
  group_by(pwsid, primary_source_code) %>%
  summarize(total_viol = length(unique(violation_id)))
# grabbing geoms for mapping: 
violations_geoms <- merge(sab[,c("pwsid")], violations, all = T)

# map of total number of violations for pwsid: 
viol_pal <- colorNumeric(
  palette = viridis::plasma(6),
  domain = violations_geoms$total_viol)
viol_map <- leaflet() %>%
  addProviderTiles(providers$CartoDB.VoyagerNoLabels, group = "Toner Lite")  %>%
  addPolygons(data = violations_geoms, 
              opacity = 9,
              color = ~viol_pal(total_viol),
              weight = 1, 
              highlightOptions = highlightOptions(color = "grey", weight = 2,
                                                  bringToFront = TRUE), 
              label = paste0("pwsid: ", violations_geoms$pwsid, 
                             " / violations: ", violations_geoms$total_viol)) %>%
    addLegend("bottomright", 
            pal = viol_pal, 
            values = violations_geoms$total_viol,
            title = "# violations",
            opacity = 1)
mapview::mapshot2(viol_map, file = "./results/pwsid_violation_map.png")

# SDWIS violations by % POC: 
violations_poc <- census_sdwis %>%
  group_by(pwsid) %>%
  summarize(total_viol = length(unique(violation_id)), 
            poc_per = mean(estimate_poc_alone_per))
ggplot(violations_poc, aes(x = poc_per, y = total_viol)) +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none")

```

_______________________________________________________________________________
Start of state-specific code: 
_______________________________________________________________________________

Loading in TX data
```{r}
# PPL data & tidying 
TX_PPL <- aws.s3::s3read_using(read.csv, object = "s3://water-team-data/clean_data/srf_project_priority_lists/web_ppl_combined_clean_v1-1.csv") %>%
  mutate(across('Project.Type', 
                str_replace, 'Other', 'General')) %>%
  filter(State == "Texas") %>%
  janitor::clean_names()

# merging census, sdwis, and ppl data into one mega data frame: 
TX_ppl_tidy <- TX_PPL
TX_cen_sdwis_ppl <- merge(census_sdwis, TX_ppl_tidy, 
                          by = "pwsid",
                          all = TRUE)
```

East TX 
```{r}
# TX counties in East Texas (done with some webpage scraping): 
east_TX <- read.csv("./data/state_raw/east_TX_counties.csv") %>%
  select(-X) %>%
  janitor::clean_names()

```