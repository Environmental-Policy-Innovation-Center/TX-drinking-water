---
title: "Texas Drinking Water Data Collating & Merging"
author: "EmmaLi Tsai"
date: "2024-02-09"
last_updated: "2024-03-21"
output: html_document
---

Packages: 
```{r packages}
library(tidyverse)
library(leaflet)
library(sf)
library(aws.s3) 
library(janitor)
library(tidycensus)
library(areal)
library(mapview)
library(httr)
library(readxl)
library(jsonlite)
library(googlesheets4)
library(googledrive)
library(rvest)
```

_______________________________________________________________________________
### Data Collating - pulling data from APIs, web links, or S3 buckets:  Data here are either pulled from S3 (see code above for how they got there) or downloaded easily from websites or APIs
_______________________________________________________________________________
## Demographic
```{r data collating}
## Demographic #################################################################
# SABs
sab_census <- aws.s3::s3read_using(st_read, 
                           object = "state-drinking-water/TX/raw/TX_sab_census.geojson",
                           bucket = "tech-team-data")

# CEJST
cejst_raw <- aws.s3::s3read_using(read.csv, object = "s3://tech-team-data/ej-data/cejest-v1/1.0-communities.csv") %>%
  janitor::clean_names() %>%
  filter(state_territory == "Texas") %>%
  select(c("census_tract_2010_id","total_threshold_criteria_exceeded",
           "total_categories_exceeded","identified_as_disadvantaged",
           "percentage_of_tract_that_is_disadvantaged_by_area","total_population"))%>%
  rename("total_population_cejst" = "total_population")


# Social Vulnerability Index 
svi <- aws.s3::s3read_using(read.csv, 
                            object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_SVI_2020.csv") %>%
  janitor::clean_names()


# Climate Vulnerability Index: 
cvi <- aws.s3::s3read_using(read.csv, 
                            object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_CVI.csv")


# TX EJ Screen: 
tx_ejscreen <- aws.s3::s3read_using(read.csv, 
                                    object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_ejscreen.csv")


# Adding this into a larger list: 
TX_demographic <- list()
TX_demographic[[1]] <- sab_census
TX_demographic[[2]] <- cejst_raw
TX_demographic[[3]] <- tx_ejscreen
TX_demographic[[4]] <- svi
TX_demographic[[5]] <- cvi
names(TX_demographic) <- c("census", "cejst", "ejscreen", "svi", "cvi")
saveRDS(TX_demographic, file="./data/raw/TX_demographic_list.RData")
put_object(
  file = "./data/raw/TX_demographic_list.RData",
  object = "/state-drinking-water/TX/clean/TX_demographic_list.RData",
  bucket = "tech-team-data",
)
```

## Environmental
```{r}
## Environmental ###############################################################
## Surface water intake locations: https://gis-tceq.opendata.arcgis.com/datasets/TCEQ::surface-intakes/api
# Pulled Jan 22 2024 
sw_intake <- GET("https://gisweb.tceq.texas.gov/arcgis/rest/services/Public/PWS/MapServer/1/query?where=1%3D1&outFields=*&outSR=4326&f=json")
sw_intake_tidy <- fromJSON(rawToChar(sw_intake$content))$features$attribute %>%
  as.data.frame() %>%
  janitor::clean_names() %>%
  rename(pwsid = pws_id) %>%
  mutate(pwsid = paste0("TX", pwsid))


## Well locations: https://www.twdb.texas.gov/mapping/gisdata/doc/well/TWDB_Groundwater.zip
# Pulled Feb 19 2024
file_loc <- "./data/raw/well_locations"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/well/TWDB_Groundwater.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
well_locations <- st_read(paste0(file_loc, "/TWDB_Groundwater.shp")) %>%
  janitor::clean_names()


## groundwater quality & levels: 
# function for reading gwdb tables, which have unique separators: 
read_gwdb_table <- function(x) {
     read.table(x, header = TRUE, fill = TRUE, sep = "|")
}
# grabbing well metadata: 
well_main <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WellMain.txt") %>%
  janitor::clean_names()

# water quality information: 
wq_major <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterQualityMajor.txt") %>%
  janitor::clean_names()
wq_minor <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterQualityMinor.txt") %>%
  janitor::clean_names()
wq_other <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterQualityOtherUnassigned.txt") %>%
  janitor::clean_names()
# let's just combine all of these 
well_wq <- rbind(wq_major, wq_minor, wq_other)

# water level information: 
wl_major <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterLevelsMajor.txt") %>%
  janitor::clean_names()
wl_minor <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterLevelsMinor.txt") %>%
  janitor::clean_names()
wl_other <- aws.s3::s3read_using(read_gwdb_table, 
                     object = "s3://tech-team-data/s3://tech-team-data/state-drinking-water/TX/raw/TX_GWDB/WaterLevelsOtherUnassigned.txt") %>%
  janitor::clean_names()
# let's just combine all of these 
well_wl <- rbind(wl_major, wl_minor, wl_other)


## Major aquifers: https://www.twdb.texas.gov/mapping/gisdata/doc/major_aquifers.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_major_aquifers"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/major_aquifers.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
maj_aquifers <- st_read(paste0(file_loc, "/NEW_major_aquifers_dd.shp")) %>%
  janitor::clean_names()


## Minor aquifers: https://www.twdb.texas.gov/mapping/gisdata/doc/minor_aquifers.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_minor_aquifers"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/minor_aquifers.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
minor_aquifers <- st_read(paste0(file_loc, "/Minor_Aquifers.shp")) %>%
  janitor::clean_names()


## Major river basins: https://www.twdb.texas.gov/mapping/gisdata/doc/Major_River_Basins_Shapefile.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_major_river_basins"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/Major_River_Basins_Shapefile.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
major_river_basins <- st_read(paste0(file_loc, "/TWDB_MRBs_2014.shp")) %>%
  janitor::clean_names()


## Existing reservoirs: https://www.twdb.texas.gov/mapping/gisdata/doc/Existing_Reservoirs.zip
# downloaded 1/23/2024
file_loc <- "./data/raw/TX_reservoirs"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/Existing_Reservoirs.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
reservoirs <- st_read(paste0(file_loc, "/TWDB_SWP2012_Major_Reservoirs.shp")) %>%
  janitor::clean_names()


## Major rivers: https://www.twdb.texas.gov/mapping/gisdata/doc/Major_Rivers_dd83.zip
# downloaded 2/14/2024
file_loc <- "./data/raw/TX_rivers"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/Major_Rivers_dd83.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
rivers <- st_read(paste0(file_loc, "/MajorRivers_dd83.shp")) %>%
  janitor::clean_names()


## Surface water 303(d) river basin impairment: https://texaswaterexplorer.tnc.org/index.html
# downloaded 1/24/2024
file_loc <- "./data/raw/TX_basin_impairment_2012"
download.file("https://texaswaterexplorer.tnc.org/downloads/Data_WQImpairment.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))
basin_impairment <- readxl::read_excel(paste0(file_loc, "/Data_WQImpairment.xlsx"), sheet = "Basins")%>%
  janitor::clean_names()


# reservoir levels - recent
recent_reservoir_levels <- aws.s3::s3read_using(read.csv, 
                                         object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_reservoir_levels_recent.csv") %>%
  select(-X)


# reservoir levels - historic
reservoir_levels_10yr <- aws.s3::s3read_using(read.csv, 
                                         object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_reservoir_levels_10yr.csv") %>%
  select(-X)


# groundwater levels
recent_well_levels <- aws.s3::s3read_using(read.csv, 
                                           object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_recent_well_levels.csv") %>%
  select(-X)


# Adding this into a larger list
# surface water: 
TX_enviro_surface <- list()
TX_enviro_surface[[1]] <- sw_intake_tidy
TX_enviro_surface[[2]] <- rivers
TX_enviro_surface[[3]] <- reservoirs
TX_enviro_surface[[4]] <- recent_reservoir_levels
TX_enviro_surface[[5]] <- reservoir_levels_10yr
TX_enviro_surface[[6]] <- major_river_basins
TX_enviro_surface[[7]] <- basin_impairment

names(TX_enviro_surface) <- c("surface_water_intakes", "rivers", "reservoirs",
                              "reservoir_levels_recent", "reservoir_levels_10yr", 
                              "major_river_basins", "impaired_basins")
saveRDS(TX_enviro_surface, file="./data/raw/TX_enviro_surface_list.RData")
put_object(
  file = "./data/raw/TX_enviro_surface_list.RData",
  object = "/state-drinking-water/TX/clean/TX_enviro_surface_list.RData",
  bucket = "tech-team-data",
)


# ground water: 
TX_enviro_ground <- list()
TX_enviro_ground[[1]] <- minor_aquifers
TX_enviro_ground[[2]] <- maj_aquifers
TX_enviro_ground[[3]] <- well_main
TX_enviro_ground[[4]] <- well_locations
TX_enviro_ground[[5]] <- recent_well_levels
TX_enviro_ground[[6]] <- well_wl
TX_enviro_ground[[7]] <- well_wq

names(TX_enviro_ground) <- c("minor_aquifers", "major_aquifers",
                             "well_metadata",
                             "well_locations", 
                             "well_levels_recent", "well_levels_historic", 
                             "well_quality_historic")
saveRDS(TX_enviro_ground, file="./data/raw/TX_enviro_ground_list.RData")
put_object(
  file = "./data/raw/TX_enviro_ground_list.RData",
  object = "/state-drinking-water/TX/clean/TX_enviro_ground_list.RData",
  bucket = "tech-team-data",
)
```

## Water Delivery System
```{r}
## Water Delivery System ######################################################
# historic use estimates: 
historic_use_estimates <- aws.s3::s3read_using(read.csv, 
                                                object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_historic_use_estimates.csv") %>%
  select(-X)
# need to fix the pwsid column: 
for(i in 1:nrow(historic_use_estimates)){
  historic_use_estimates$pwsid[i] <- paste0("TX", 
                                            paste(rep(0, (7-nchar(historic_use_estimates$tceq_pws_code[i]))), 
                                                  collapse = ""), 
                                            historic_use_estimates$tceq_pws_code[i], 
                                            collapse = "")
}


# water loss audits: https://www3.twdb.texas.gov/apps/reports/WLA/SummaryAuditsByCategory
# downloaded Jan 24 2024
# vector of cleaned column names 
waterlosscolnames <- c("Name of Utility",
                       "Real Loss GMD (<32 conn/mi)",
                       "Real Loss GCD",
                       "Apparent Loss GCD",
                       "Water Loss GCD",
                       "ILI (>= 3,000 connections)", 
                       "Total GPCD", 
                       "GPCD Loss",
                       "Real Loss Cost in dollars",
                       "Apparent Loss Cost in dollars","Year")

#making empty dataframe 
waterloss_all <- setNames(data.frame(matrix(ncol = length(waterlosscolnames), nrow = 0)), waterlosscolnames)

year <- 2010 
# ! edit this when adding additional years 
stopyear <- 2022  
while(year <= stopyear)
{
print(year)
  
waterloss_raw <- aws.s3::s3read_using(read_csv, 
                               object = paste0("s3://tech-team-data/state-drinking-water/TX/raw/TCEQ_Loss_Audits/SummaryAuditsByCategory_",year,".csv"))

waterloss <- waterloss_raw %>%
             select(1:10)

colnames(waterloss) <- waterlosscolnames

waterloss <- waterloss %>%
             janitor::clean_names()%>%
             filter(name_of_utility != "UtilityName")%>%
             mutate(Year = year)

waterloss_all <- rbind(waterloss_all,waterloss)
year <- year + 1
}


## 2022 TX State Water Plan: https://2022.texasstatewaterplan.org/statewide?
# downloaded Jan 29 2024
pop <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/population") %>%
  janitor::clean_names()
demands <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/demands") %>%
  janitor::clean_names()
supp <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/supplies") %>%
  janitor::clean_names()
needs <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/needs") %>%
  janitor::clean_names()
strat <- read.csv("https://2022.texasstatewaterplan.org/download/statewide/strategies") %>%
  janitor::clean_names()


# TCEQ DWW system data: 
# NOTE: there is A TON more information in s3 for us to pull like enforcement 
# actions, etc. - starting the structure here for grabbing that information. 
# Requires pulling  from two different webscraping events and locating pwsids that 
# are community water systems: 
water_details_dec17 <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231217/WaterSystemDetail/TX_Water System Details_20231217_20231229203427.csv") 
water_details_dec3rd <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231203/WaterSystemDetail/TX_Water System Details_20231203.csv") 
combined_details <- rbind(water_details_dec17, water_details_dec3rd)
water_details <- pivot_wider(combined_details, names_from = "Type", values_from = "Value") %>%
  janitor::clean_names()
# grabbing just community water systems (TCEQ has 10k that are not CWS): 
tceq_cws <- water_details %>%
  filter(system_type == "C") 

# pulling violations: 
viol_dec17 <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231217/Violations/TX_Individual Violations_20231217_20231229203427.csv") 
viol_dec3rd <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_TCEQ_DWW/TX_20231203/Violations/TX_Individual Violations_20231203.csv") 
combined_viols <- rbind(viol_dec17, viol_dec3rd) %>%
  janitor::clean_names() %>%
  filter(water_system_no %in% tceq_cws$water_system_no) %>% 
  rename(pwsid = water_system_no)


# TX SDWIS violations: 
# Pulled Jan 24 2024
sdwis <- aws.s3::s3read_using(read.csv, 
                              object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_sdwis.csv") %>%
  select(-X)


## TX water restrictions: https://www.tceq.texas.gov/drinkingwater/trot/droughtw.html
# Pulled March 5 2024: 
url <- "https://www.tceq.texas.gov/drinkingwater/trot/droughtw.html"
# web scraping: 
tx_water_limits_list <- url %>% 
  read_html() %>% 
  html_nodes("table") %>% 
  html_table(fill = T) 
# yanking out list
tx_water_restrictions <- tx_water_limits_list[[1]] 
# tidying and fixing pwsid column: 
tx_water_restrictions <- tx_water_restrictions %>%
  janitor::clean_names() %>%
  rename(pwsid = pws_id) %>%
  mutate(date_notified = as.Date(date_notified, tryFormats = c("%m/%d/%Y")))
for(i in 1:nrow(tx_water_restrictions)){
  tx_water_restrictions$pwsid[i] <- paste0("TX", paste(rep(0, (7-nchar(tx_water_restrictions$pwsid[i]))), 
                                                        collapse = ""), 
                                     tx_water_restrictions$pwsid[i], collapse = "")
}


## TX Boil Water Notices - via FOIA 
# Received from TCEQ on 4/17/2024
bwn_f1 <- aws.s3::s3read_using(read_excel, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_BWN/TX_bwn_detailed_f1.xlsx") %>%
  janitor::clean_names()
bwn_f2 <- aws.s3::s3read_using(read_excel, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_BWN/TX_bwn_simple_f2.xlsx") %>%
  janitor::clean_names()


# throwing this into a list: 
TX_water_delivery <- list()
TX_water_delivery[[1]] <- sdwis
TX_water_delivery[[2]] <- combined_viols
TX_water_delivery[[3]] <- historic_use_estimates
TX_water_delivery[[4]] <- waterloss_all
TX_water_delivery[[5]] <- tx_water_restrictions
TX_water_delivery[[6]] <- pop 
TX_water_delivery[[7]] <- demands
TX_water_delivery[[8]] <- supp
TX_water_delivery[[9]] <- needs
TX_water_delivery[[10]] <- strat
TX_water_delivery[[11]] <- bwn_f1
TX_water_delivery[[12]] <- bwn_f2

names(TX_water_delivery) <- c("sdwis", "tceq_viols",
                              "historic_use_estimates",
                              "water_loss", "water_restrictions",
                              "TXSWP_pop", "TXSWP_demands",
                              "TXSWP_supp", "TXSWP_needs", "TXSWP_strat", 
                              "bwn_f1", "bwn_f2")

saveRDS(TX_water_delivery, file="./data/raw/TX_water_delivery_list.RData")
put_object(
  file = "./data/raw/TX_water_delivery_list.RData",
  object = "/state-drinking-water/TX/clean/TX_water_delivery_list.RData",
  bucket = "tech-team-data",
)
```

## Financial 
```{r}
## Financial ##################################################################
# Combining and adding full ppl lists from SFY 14-24 - having to do them 
# in chunks because of slight discrepancies in column names 
# added March 5th 2024: 
# rbinding SRF ppls on aws that Phil scraped(thank you, Phil!): 
ppl15 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy15-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  rename(gpr = x, 
         disadv = disadvantaged) %>%
  mutate(sfy = "sfy15")
ppl16 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy16-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy16")
ppl17 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy17-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy17") %>%
  rename(phase_s = eligible_phase_s)
# rbinding 15-17
ppl15_17 <- rbind(ppl15, ppl16, ppl17)

ppl19 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy19-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy19") 
ppl20 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy20-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy20") %>%
  rename(phase_s = eligible_phase_s)
ppl21 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy21-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy21") %>%
  rename(phase_s = eligible_phase_s)
# binding 15-21
ppl15_21 <- rbind(ppl15_17, ppl19, ppl20, ppl21)

ppl22 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy22-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy22") 
ppl23 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy23-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy23") 
ppl24 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy24-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  mutate(sfy = "sfy24") %>%
  rename(phase_s = eligible_phase_s)
# binding 15-24
ppl15_24 <- rbind(ppl15_21, ppl22, ppl23, ppl24)

# adding ppl14 last since the columns are an odd order: 
ppl14 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-dwsrf-ppls/tx-dw-sfy14-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  select(-estimated_construction_start) %>%
  rename(disadv = disadvantaged) %>%
  mutate(sfy = "sfy14")
ppl14 <- ppl14[,names(ppl15_24)]

# bringing it all together! 
ppl14_24 <- rbind(ppl14, ppl15_24)


## Pulling data from CWSRFs: 
# March 15, 2024
sfy14 <- aws.s3::s3read_using(read.csv, 
                     object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy14-ppl-invited.csv") %>%
  janitor::clean_names()%>%
  select(-c("estimated_construction_start", "epa_cat")) %>%
  mutate(sfy = "sfy14")%>%
  rename(disadv = disadvantaged)
sfy15 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy15-ppl-invited.csv") %>%
  janitor::clean_names()  %>%
  mutate(sfy = "sfy15")%>%
  rename(disadv = disadvantaged)
sfy16 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy16-ppl-invited.csv") %>%
  janitor::clean_names()%>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy16")
sfy17 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy17-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy17")
# missing sfy18 :-(
sfy19 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy19-ppl-invited.csv") %>%
  janitor::clean_names()%>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy19")
# SFY 20 doesn't have information on which pwsids were invited: 
sfy20 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy20-ppl.csv") %>%
  janitor::clean_names() %>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy20") %>%
  mutate(invited = "no_information_sfy20")%>%
  relocate(invited, .after = rank)
sfy21 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy21-ppl-invited.csv") %>%
  janitor::clean_names()%>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy21")
sfy22 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy22-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy22") 
sfy23 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy23-ppl-invited.csv") %>%
  janitor::clean_names()%>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy23")
sfy24 <- aws.s3::s3read_using(read.csv, 
                              object = "s3://water-team-data/clean_data/srf_project_priority_lists/texas-cwsrf-ppls/tx-cw-sfy24-ppl-invited.csv") %>%
  janitor::clean_names() %>%
  select(-c("epa_cat"))%>%
  mutate(sfy = "sfy24") 
# bringing it all together! 
cwsrf_ppl14_24 <- rbind(sfy14, sfy15, sfy16, sfy17, sfy19, sfy20, sfy21, sfy22, sfy23, sfy24)


## Danielle's SRF analysis: had to download it using a different method because 
# it's an excel file that's stored on google drive - so it's not actually a 
# google sheet. 
# downloaded 2/1/2024 
URL <- "https://docs.google.com/spreadsheets/d/1kUHLdzFjwmKx4r6YTu2qjxwNuuG5JeUi/edit#gid=252194392"
dl <- googledrive::drive_download(
  as_id(URL),
  path = 'temp1.xlsx', 
  type = "xlsx")
cwsrf_by_rank <- readxl::read_excel('temp1.xlsx', sheet = "CWSRF By Rank", 
                                        skip = 1, col_names = TRUE) %>%
  janitor::clean_names() %>%
  # selecting columns of interest: 
  select(rank:sfy)
file.remove("temp1.xlsx")

## TX DAC - whether they are likely to qualify for DAC status - requires pulling water affordability data and combining this with mhi
TX_DAC <- aws.s3::s3read_using(st_read, 
                               object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_DAC.geojson")

# throwing this into a list: 
TX_financial <- list()
TX_financial[[1]] <- TX_DAC
TX_financial[[2]] <- ppl14_24
TX_financial[[3]] <- cwsrf_by_rank
TX_financial[[4]] <- cwsrf_ppl14_24

names(TX_financial) <- c("TX_affordability_DAC", "TX_PPL_SFY14_24",
                         "SRF_analysis", "CWSRF_TX_PPL_SFY14_24")

saveRDS(TX_financial, file="./data/raw/TX_financial_list.RData")
put_object(
  file = "./data/raw/TX_financial_list.RData",
  object = "/state-drinking-water/TX/clean/TX_financial_list.RData",
  bucket = "tech-team-data",
)
```